{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# for using collab without an environment\n",
        "# !pip install datasets\n",
        "# !pip install wandb"
      ],
      "metadata": {
        "id": "94DTcky5tLlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the results of SentEval:"
      ],
      "metadata": {
        "id": "in4n_fiQkMr7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zZvSwLgzzOzo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "result_path = './results/'\n",
        "with open(result_path + 'baseline', 'r') as f:\n",
        "    result_baseline = json.load(f)\n",
        "with open(result_path + 'lstm', 'r') as f:\n",
        "    result_lstm = json.load(f)\n",
        "with open(result_path + 'bilstm', 'r') as f:\n",
        "    result_bilstm = json.load(f)\n",
        "with open(result_path + 'poollstm', 'r') as f:\n",
        "    result_poollstm = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading pretrained models and evaluating them on eval and test dataset:"
      ],
      "metadata": {
        "id": "_2VTAoiEkn-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from model import USC\n",
        "from dataset import get_data\n",
        "from train import eval\n",
        "\n",
        "# checkpoints need to be loaded with GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "checkpoint_path_base = './saves/baseline/checkpoint.pth'\n",
        "checkpoint_path_lstm = './saves/lstm/checkpoint.pth'\n",
        "checkpoint_path_bilstm = './saves/bilstm/checkpoint.pth'\n",
        "checkpoint_path_poollstm = './saves/poollstm/checkpoint.pth'\n",
        "\n",
        "\n",
        "train_loader, val_loader, test_loader, vectors, new_vocab = get_data(save_path=None, glove_path=None)\n",
        "model_baseline = USC(encoder='baseline', vocab_size=len(vectors), sentence_length=150, vector_embeddings=vectors, embedding_dim=300,)\n",
        "model_lstm = USC(encoder='lstm', vocab_size=len(vectors), sentence_length=150, vector_embeddings=vectors, embedding_dim=300,)\n",
        "model_bilstm = USC(encoder='bilstm', vocab_size=len(vectors), sentence_length=150, vector_embeddings=vectors, embedding_dim=300,)\n",
        "model_poollstm = USC(encoder='poollstm', vocab_size=len(vectors), sentence_length=150, vector_embeddings=vectors, embedding_dim=300,)\n",
        "\n",
        "model_baseline = model_baseline.to(device)\n",
        "model_lstm = model_lstm.to(device)\n",
        "model_bilstm = model_bilstm.to(device)\n",
        "model_poollstm = model_poollstm.to(device)\n",
        "\n",
        "model_baseline.load_state_dict(torch.load(checkpoint_path_base), strict=False)\n",
        "model_lstm.load_state_dict(torch.load(checkpoint_path_lstm), strict=False)\n",
        "model_bilstm.load_state_dict(torch.load(checkpoint_path_bilstm), strict=False)\n",
        "model_poollstm.load_state_dict(torch.load(checkpoint_path_poollstm), strict=False)\n",
        "\n",
        "# just small eval (~2 min)\n",
        "test_acc_base = eval(model_baseline, test_loader)\n",
        "val_acc_base = eval(model_baseline, val_loader)\n",
        "\n",
        "test_acc_lstm = eval(model_lstm, test_loader)\n",
        "val_acc_lstm = eval(model_lstm, val_loader)\n",
        "\n",
        "test_acc_bilstm = eval(model_bilstm, test_loader)\n",
        "val_acc_bilstm = eval(model_bilstm, val_loader)\n",
        "\n",
        "test_acc_poollstm = eval(model_poollstm, test_loader)\n",
        "val_acc_poollstm = eval(model_poollstm, val_loader)\n",
        "\n",
        "test_acc = [test_acc_base.cpu().item(), test_acc_lstm.cpu().item(), test_acc_bilstm.cpu().item(), test_acc_poollstm.cpu().item()]\n",
        "val_acc = [val_acc_base.cpu().item(), val_acc_lstm.cpu().item(), val_acc_bilstm.cpu().item(), val_acc_poollstm.cpu().item()]"
      ],
      "metadata": {
        "id": "xPI5wlcakuia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304e3d5a-ae50-45a2-d9b1-81ba6ad03e39"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute micro and macro metrics:"
      ],
      "metadata": {
        "id": "kc8lATJTkRBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "datasets = [result_baseline, result_lstm, result_bilstm, result_poollstm]\n",
        "micro = []\n",
        "macro = []\n",
        "\n",
        "for dataset in datasets:\n",
        "  table_data = []\n",
        "  for task in dataset:\n",
        "      if 'devacc' in dataset[task]:\n",
        "          table_data.append([dataset[task]['devacc'], dataset[task]['ndev']])\n",
        "  table_data = np.array(table_data)\n",
        "  micro.append(np.mean(table_data[:,0]))\n",
        "  macro.append(np.sum(table_data[:,0]*table_data[:,1]) / np.sum(table_data[:,1]))"
      ],
      "metadata": {
        "id": "bTk_xTp0goE2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# recreate the table 3\n",
        "dim = [1200, 2048, 4096, 4096]\n",
        "\n",
        "data = {'dim': dim,\n",
        "        'dev_acc': val_acc,\n",
        "        'test_acc': test_acc,\n",
        "        'micro': micro,\n",
        "        'macro': macro}\n",
        "\n",
        "df = pd.DataFrame(data=data, index=['Base', 'LSTM', 'BiLSTM', 'BiLSTM_Max'])\n",
        "format_func = lambda x: f\"{x:.2f}\" if isinstance(x, (float)) else x\n",
        "df = df.applymap(format_func)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iteeafsWsMja",
        "outputId": "f17a52ba-c238-41b3-e717-cef91602b13d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             dim dev_acc test_acc  micro  macro\n",
            "Base        1200    0.54     0.53  74.86  76.44\n",
            "LSTM        2048    0.40     0.40  59.25  62.17\n",
            "BiLSTM      4096    0.62     0.62  74.17  75.80\n",
            "BiLSTM_Max  4096    0.76     0.77  79.98  81.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create table showing performance on downstream tasks of SentEval:"
      ],
      "metadata": {
        "id": "DILn7AymkZg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [result_baseline, result_lstm, result_bilstm, result_poollstm]\n",
        "tasks = set()\n",
        "for dataset in datasets:\n",
        "    tasks.update(dataset.keys())\n",
        "tasks = sorted(tasks)\n",
        "df = pd.DataFrame(columns=tasks)\n",
        "for i, dataset in enumerate(datasets):\n",
        "    row = {}\n",
        "    for task in tasks:\n",
        "        if task in dataset and 'devacc' in dataset[task]:\n",
        "            row[task] = dataset[task]['devacc']\n",
        "    df.loc[i] = row\n",
        "df.drop('STS14', axis=1, inplace=True)\n",
        "df = df.rename(columns={'SICKEntailment': 'SICKE'})\n",
        "df = df.rename(index={0: 'Baseline', 1: 'LSTM', 2: 'BiLSTM', 3: 'BiLSTM_MAX'})\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43x7ZXtidn5N",
        "outputId": "1b3090ce-0f6d-4522-883e-619e792e067d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               CR   MPQA     MR   MRPC  SICKE   SST2   SUBJ   TREC\n",
            "Baseline    75.23  75.95  74.20  72.69   72.4  77.06  89.37  62.01\n",
            "LSTM        65.22  77.62  56.62  67.54   56.4  57.22  70.46  22.93\n",
            "BiLSTM      71.30  84.16  66.38  68.47   75.4  70.87  83.47  73.31\n",
            "BiLSTM_MAX  79.91  85.52  75.52  72.67   81.4  80.39  91.30  73.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answering the assignment question and demonstrating models:"
      ],
      "metadata": {
        "id": "aV12ZN0cA1i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import prepare_sentence_batch\n",
        "\n",
        "model = model_poollstm\n",
        "\n",
        "p1 = ['Two men sitting in the sun']\n",
        "h1 = ['Nobody is sitting in the shade']\n",
        "\n",
        "# Label - Neutral (likely predicts contradiction)\n",
        "\n",
        "p2 = ['A man is walking a dog']\n",
        "h2 = ['No cat is outside']\n",
        "\n",
        "# Label - Neutral (likely predicts contradiction)\n",
        "\n",
        "p1, p1_len = prepare_sentence_batch(p1, new_vocab, 150)\n",
        "h1, h1_len = prepare_sentence_batch(h1, new_vocab, 150)\n",
        "p1 = p1.to(device)\n",
        "h1 = h1.to(device)\n",
        "p1_len = p1_len.squeeze(1).to(device)\n",
        "h1_len = h1_len.squeeze(1).to(device)\n",
        "\n",
        "prediction1 = torch.argmax(model(p1, p1_len, h1, h1_len), dim=1)\n",
        "\n",
        "p2, p2_len = prepare_sentence_batch(p2, new_vocab, 150)\n",
        "h2, h2_len = prepare_sentence_batch(h2, new_vocab, 150)\n",
        "p2 = p2.to(device)\n",
        "h2 = h2.to(device)\n",
        "p2_len = p2_len.squeeze(1).to(device)\n",
        "h2_len = h2_len.squeeze(1).to(device)\n",
        "\n",
        "prediction2 = torch.argmax(model(p2, p2_len, h2, h2_len), dim=1)\n",
        "\n",
        "print('Labels: 0: entailment, 1: neutral, 2: contradiction')\n",
        "print(f'Prediction for first example is: {prediction1}')\n",
        "print(f'Prediction for second example is: {prediction2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOI44s6JwGWP",
        "outputId": "eaa26cf0-8cf6-4cfe-d90d-83921bd66d7e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: 0: entailment, 1: neutral, 2: contradiction\n",
            "Prediction for first example is: tensor([1], device='cuda:0')\n",
            "Prediction for first example is: tensor([1], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this tricky example our model is acctually correct. Next we will test with a weaker model:"
      ],
      "metadata": {
        "id": "Od9roVgtDeuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_baseline\n",
        "\n",
        "p1 = ['Two men sitting in the sun']\n",
        "h1 = ['Nobody is sitting in the shade']\n",
        "\n",
        "# Label - Neutral (likely predicts contradiction)\n",
        "\n",
        "p2 = ['A man is walking a dog']\n",
        "h2 = ['No cat is outside']\n",
        "\n",
        "# Label - Neutral (likely predicts contradiction)\n",
        "\n",
        "p1, p1_len = prepare_sentence_batch(p1, new_vocab, 150)\n",
        "h1, h1_len = prepare_sentence_batch(h1, new_vocab, 150)\n",
        "p1 = p1.to(device)\n",
        "h1 = h1.to(device)\n",
        "p1_len = p1_len.squeeze(1).to(device)\n",
        "h1_len = h1_len.squeeze(1).to(device)\n",
        "\n",
        "prediction1 = torch.argmax(model(p1, p1_len, h1, h1_len), dim=1)\n",
        "\n",
        "p2, p2_len = prepare_sentence_batch(p2, new_vocab, 150)\n",
        "h2, h2_len = prepare_sentence_batch(h2, new_vocab, 150)\n",
        "p2 = p2.to(device)\n",
        "h2 = h2.to(device)\n",
        "p2_len = p2_len.squeeze(1).to(device)\n",
        "h2_len = h2_len.squeeze(1).to(device)\n",
        "\n",
        "prediction2 = torch.argmax(model(p2, p2_len, h2, h2_len), dim=1)\n",
        "\n",
        "print('Labels: 0: entailment, 1: neutral, 2: contradiction')\n",
        "print(f'Prediction for first example is: {prediction1}')\n",
        "print(f'Prediction for second example is: {prediction2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CB6h-o6DsYe",
        "outputId": "597f54c1-bf3e-4fb6-ee3f-1eaf925fd105"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: 0: entailment, 1: neutral, 2: contradiction\n",
            "Prediction for first example is: tensor([1], device='cuda:0')\n",
            "Prediction for first example is: tensor([2], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time our model is wrong for the second example. It predicts contradiction instead of neutral. This is understandable as it makes sense as the embeddings for dog and cat are likely to be close. And one is outside while the hypothesis states that the other (which the model assumes to be almost the same) is not. The same is likely for the first example but here our model is correct.\n"
      ],
      "metadata": {
        "id": "ZkQvKsTXDy9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Research question\n"
      ],
      "metadata": {
        "id": "yi5n6xqBFezz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to know if this model can handle race appropiatley with two short examples:"
      ],
      "metadata": {
        "id": "cbQ7vMoMFh4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_poollstm\n",
        "\n",
        "p1 = ['No man is outside']\n",
        "h1 = ['Two afro-americans are sitting outside']\n",
        "\n",
        "# Label - contradiction\n",
        "\n",
        "p2 = ['All man die']\n",
        "h2 = ['White people are immortal']\n",
        "\n",
        "# Label - contradiction\n",
        "\n",
        "p1, p1_len = prepare_sentence_batch(p1, new_vocab, 150)\n",
        "h1, h1_len = prepare_sentence_batch(h1, new_vocab, 150)\n",
        "p1 = p1.to(device)\n",
        "h1 = h1.to(device)\n",
        "p1_len = p1_len.squeeze(1).to(device)\n",
        "h1_len = h1_len.squeeze(1).to(device)\n",
        "\n",
        "prediction1 = torch.argmax(model(p1, p1_len, h1, h1_len), dim=1)\n",
        "\n",
        "p2, p2_len = prepare_sentence_batch(p2, new_vocab, 150)\n",
        "h2, h2_len = prepare_sentence_batch(h2, new_vocab, 150)\n",
        "p2 = p2.to(device)\n",
        "h2 = h2.to(device)\n",
        "p2_len = p2_len.squeeze(1).to(device)\n",
        "h2_len = h2_len.squeeze(1).to(device)\n",
        "\n",
        "prediction2 = torch.argmax(model(p2, p2_len, h2, h2_len), dim=1)\n",
        "\n",
        "print('Labels: 0: entailment, 1: neutral, 2: contradiction')\n",
        "print(f'Prediction for first example is: {prediction1}')\n",
        "print(f'Prediction for second example is: {prediction2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjvlXQbeFqGQ",
        "outputId": "d4baeca0-2ba8-4148-adb7-266c61564ae7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: 0: entailment, 1: neutral, 2: contradiction\n",
            "Prediction for first example is: tensor([1], device='cuda:0')\n",
            "Prediction for first example is: tensor([1], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interestingly enough it is wrong on both accounts. I will pose the same question without race differences again:"
      ],
      "metadata": {
        "id": "1BLliZ29F9eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_poollstm\n",
        "\n",
        "p1 = ['No man is outside']\n",
        "h1 = ['Two persons are sitting outside']\n",
        "\n",
        "# Label - contradiction\n",
        "\n",
        "p2 = ['All man die']\n",
        "h2 = ['People are immortal']\n",
        "\n",
        "# Label - contradiction\n",
        "\n",
        "p1, p1_len = prepare_sentence_batch(p1, new_vocab, 150)\n",
        "h1, h1_len = prepare_sentence_batch(h1, new_vocab, 150)\n",
        "p1 = p1.to(device)\n",
        "h1 = h1.to(device)\n",
        "p1_len = p1_len.squeeze(1).to(device)\n",
        "h1_len = h1_len.squeeze(1).to(device)\n",
        "\n",
        "prediction1 = torch.argmax(model(p1, p1_len, h1, h1_len), dim=1)\n",
        "\n",
        "p2, p2_len = prepare_sentence_batch(p2, new_vocab, 150)\n",
        "h2, h2_len = prepare_sentence_batch(h2, new_vocab, 150)\n",
        "p2 = p2.to(device)\n",
        "h2 = h2.to(device)\n",
        "p2_len = p2_len.squeeze(1).to(device)\n",
        "h2_len = h2_len.squeeze(1).to(device)\n",
        "\n",
        "prediction2 = torch.argmax(model(p2, p2_len, h2, h2_len), dim=1)\n",
        "\n",
        "print('Labels: 0: entailment, 1: neutral, 2: contradiction')\n",
        "print(f'Prediction for first example is: {prediction1}')\n",
        "print(f'Prediction for second example is: {prediction2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXLI6v54GPRo",
        "outputId": "e545c8a5-635e-4f72-b8c0-3efb66ed88be"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: 0: entailment, 1: neutral, 2: contradiction\n",
            "Prediction for first example is: tensor([1], device='cuda:0')\n",
            "Prediction for first example is: tensor([1], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same result. This might mean that my model can not pick up the contradiction. Rather than having to do anything with race."
      ],
      "metadata": {
        "id": "ZLPXeI1kGYZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will try again with more continous examples:"
      ],
      "metadata": {
        "id": "aZMfzHnPGnpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_poollstm\n",
        "\n",
        "p1 = ['Two black man walk outside']\n",
        "h1 = ['The man talk deeply']\n",
        "\n",
        "# Label - entailment\n",
        "\n",
        "p2 = ['Two white man walk outside']\n",
        "h2 = ['The man talk deeply']\n",
        "\n",
        "# Label - entailment\n",
        "\n",
        "p1, p1_len = prepare_sentence_batch(p1, new_vocab, 150)\n",
        "h1, h1_len = prepare_sentence_batch(h1, new_vocab, 150)\n",
        "p1 = p1.to(device)\n",
        "h1 = h1.to(device)\n",
        "p1_len = p1_len.squeeze(1).to(device)\n",
        "h1_len = h1_len.squeeze(1).to(device)\n",
        "\n",
        "prediction1 = torch.argmax(model(p1, p1_len, h1, h1_len), dim=1)\n",
        "\n",
        "p2, p2_len = prepare_sentence_batch(p2, new_vocab, 150)\n",
        "h2, h2_len = prepare_sentence_batch(h2, new_vocab, 150)\n",
        "p2 = p2.to(device)\n",
        "h2 = h2.to(device)\n",
        "p2_len = p2_len.squeeze(1).to(device)\n",
        "h2_len = h2_len.squeeze(1).to(device)\n",
        "\n",
        "prediction2 = torch.argmax(model(p2, p2_len, h2, h2_len), dim=1)\n",
        "\n",
        "print('Labels: 0: entailment, 1: neutral, 2: contradiction')\n",
        "print(f'Prediction for first example is: {prediction1}')\n",
        "print(f'Prediction for second example is: {prediction2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5_TZAxuGvsN",
        "outputId": "c9b6f441-25ee-48e6-e8fb-744de5075fcf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: 0: entailment, 1: neutral, 2: contradiction\n",
            "Prediction for first example is: tensor([1], device='cuda:0')\n",
            "Prediction for first example is: tensor([1], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer is wrong on both accounts"
      ],
      "metadata": {
        "id": "u_B1n4OBHTDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again with another model:"
      ],
      "metadata": {
        "id": "LLQsWsFjHKI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_baseline\n",
        "\n",
        "p1 = ['Two black man walk outside']\n",
        "h1 = ['The man talk deeply']\n",
        "\n",
        "# Label - contradiction\n",
        "\n",
        "p2 = ['Two white man walk outside']\n",
        "h2 = ['The man talk deeply']\n",
        "\n",
        "# Label - contradiction\n",
        "\n",
        "p1, p1_len = prepare_sentence_batch(p1, new_vocab, 150)\n",
        "h1, h1_len = prepare_sentence_batch(h1, new_vocab, 150)\n",
        "p1 = p1.to(device)\n",
        "h1 = h1.to(device)\n",
        "p1_len = p1_len.squeeze(1).to(device)\n",
        "h1_len = h1_len.squeeze(1).to(device)\n",
        "\n",
        "prediction1 = torch.argmax(model(p1, p1_len, h1, h1_len), dim=1)\n",
        "\n",
        "p2, p2_len = prepare_sentence_batch(p2, new_vocab, 150)\n",
        "h2, h2_len = prepare_sentence_batch(h2, new_vocab, 150)\n",
        "p2 = p2.to(device)\n",
        "h2 = h2.to(device)\n",
        "p2_len = p2_len.squeeze(1).to(device)\n",
        "h2_len = h2_len.squeeze(1).to(device)\n",
        "\n",
        "prediction2 = torch.argmax(model(p2, p2_len, h2, h2_len), dim=1)\n",
        "\n",
        "print('Labels: 0: entailment, 1: neutral, 2: contradiction')\n",
        "print(f'Prediction for first example is: {prediction1}')\n",
        "print(f'Prediction for second example is: {prediction2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJugA5YSHODx",
        "outputId": "ab9e994f-ce89-48df-f4fd-6c96b0e9c7f3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: 0: entailment, 1: neutral, 2: contradiction\n",
            "Prediction for first example is: tensor([0], device='cuda:0')\n",
            "Prediction for first example is: tensor([0], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer is correct for the baseline model. This is interesting as it means that although the poollstm model performs better it can also be worse. I found no bias in race but further study is needed."
      ],
      "metadata": {
        "id": "h3S_gOIbHb1M"
      }
    }
  ]
}